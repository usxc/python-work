import requests
from bs4 import BeautifulSoup
import urllib
from urllib.parse import urljoin
from pathlib import Path
import time


def getImageUrl(url):
    # Webページを取得して解析する
    html = requests.get(url)
    soup = BeautifulSoup(html.content, "html.parser")

    entry = soup.find(class_="entry")
    # print(entry)

    img = entry.find("img")
    # print(img)

    src = img.get("src")
    # print(src)

    # URLにHTTPSがない場合は追加する
    if not "https:" in src:
        src = "https:" + src

    return src


def downloadImage(url, dir):
    print("imaeg >> " + url)
    filename = url.split("/")[-1]
    fullpath = dir.joinpath(filename)

    imgdata = requests.get(url)
    with open(fullpath, mode="wb") as f:
        f.write(imgdata.content)


def readResultHtml(targetUrl, urls):
    html = requests.get(targetUrl)
    soup = BeautifulSoup(html.content, "html.parser")

    result = soup.find(class_="widget Blog")

    # すべてのaタグを検索して、リンクを表示する
    for element in result.find_all("a"):
        url = urljoin(targetUrl, element.get("href"))

        # 正規表現を使えばもっとシンプルに書ける
        if "/blog-post_" in url:
            # if "/blog-post_" in url and not "#comment" in url:

            urls.append(url)  # リストを使う場合の追加
            # urls.add(url)	# 重複を除外する セットを使う場合

        else:
            pass
            # print("NG-URL >> " + url)


# ここから本題

keyword = urllib.parse.quote("寿司")
step = 5
start = step * 0

# Webページを取得して解析する
resultUrl = f"https://www.irasutoya.com/search?q={keyword}&max-results={step}&start={start}&by-date=false"

urls = []  # リスト
# urls = set() # セット

# 詳細画面一覧のURLリストが作成されます
readResultHtml(resultUrl, urls)

# ダウンロード先ディレクトリの作成
dldir = Path(f"download")
dldir.mkdir(exist_ok=True)

for url in urls:
    imageUrl = getImageUrl(url)
    print(url + "   image>> " + imageUrl)
    downloadImage(imageUrl, dldir)
    time.sleep(1)
